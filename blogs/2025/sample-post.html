<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Introduction to Score-based Generative Models - Vignesh Srinivasan</title>
    <meta name="author" content="Vignesh Srinivasan">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="../../images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="../../stylesheet.css">
    <!-- Add MathJax for LaTeX support -->
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
  </head>

  <body>
    <div class="container">
      <!-- Navigation Header -->
      <header>
        <h1>Vignesh Srinivasan</h1>
        <nav>
          <ul>
            <li><a href="../../index.html#about">About</a></li>
            <li><a href="../../index.html#publications">Publications</a></li>
            <li><a href="../../index.html#blogs">Blog</a></li>
          </ul>
        </nav>
      </header>

      <article class="blog-post">
        <header class="post-header">
          <h1>Scaling Recipes for Training Large Neural Networks</h1>
          <p class="post-meta">April 15, 2025</p>
        </header>

        <div class="post-content">
          <p>
            This blog post gives a brief overview of how to bake a bigger loaf of bread and also how to scale training of large neural networks. 
          </p>

          <div style="text-align: center;">
            <img src="../images/sourdough_bread.png" alt="Baking bread is not very different from training neural networks" class="blog-image" style="width: 50%;">
          </div>
          
          
          <h2>Why scale?</h2>
          <p>
            A bigger loaf of bread is better than a smaller one for many reasons: it tastes better, it has unique flavors as well as textures.
            Scaling is important for neural networks because it improves the performance of the model on the given task, be it classification, language modeling, or image generation.
            Scalability is limited by all three aspects: data, compute, and model size.
          </p>

          <div style="text-align: center;">
            <img src="../images/scaling_law_kaplan.png" alt="Analogy between scaling neural networks and baking bread" class="blog-image">
          </div>
          <p class="image-caption">
            Figure from <a href="https://arxiv.org/abs/2001.08361">Scaling Laws for Neural Language Models</a> by Kaplan et al. 
            showing how model performance improves with compute, data and model size.
          </p>

          <h2>Challenges in scaling</h2>
          <p>
            It is easy to understand that scaling is important, but far difficult to scale in practice.

            The hardware (oven size) is often limited to a given size. This leaves us with the amount of flour that can be scaled within the oven.
            Yet, simply increasing the amount of flour, can result in a bread that is too dense or undercooked.

            Similarly, given fixed compute resources and data available to us, we can scale by increasing the model size.
            Although, now the only parameter that we control is the model size, it is still a very challenging task to scale the model effectively.
            Bigger models are very finicky and are very expensive ($$$) to explore. 
          </p>

          <h2>Scaling recipes</h2>
          <p>
            This is where μ-Parametrization (μP) comes in. 
            μP is a principled approach to scaling neural networks that ensures consistent training dynamics across different model sizes. 
            By following simple guidelines, μP helps maintain stable gradients and convergence properties as models grow larger.
          </p>
          <ol>
            <li>Initialize the weights with unit scale by a factor proportional to the width of the network</li>
            <li>Make sure the weights are properly scaled after gradient updates by tuning the learning rate</li>
          </ol>
          
          This is essentially a recipe describing the hydration level, time to proof the dough, and time to bake the bread.
          All that is left is to bake the bread and see the results!

          <h2>Results</h2>
          <p>
            We test μP on the MNIST dataset.
            The hyperparameters (HP) tuned here are the model width and learning rate.
          </p>
          <div style="text-align: center;">
            <img src="../images/sweep_metrics_mup.png" alt="mup" class="blog-image">
          </div>
          
          <div style="text-align: center;">
            <img src="../images/sweep_metrics_sp.png" alt="sp" class="blog-image">
          </div>

          <h2>Conclusion</h2>
          <p>
            μP is a simple and effective recipe for scaling neural networks.
            It ensures consistent training dynamics across different model sizes and shows convergence at the same HP setting for all model sizes.
            If you plan to train a large neural network, first try out a large number of small models to find the HP setting that works best for you.
            Transfer the best HP setting to your large model.

            Similarly, if you plan to bake a big loaf of bread, 
            first try out a large number of small bread sizes to find the recipe that works best for you!
          </p>

          Try out the recipe yourself!
          <a href="https://github.com/VigneshSrinivasan10/scaling-recipes/">Scaling Recipes</a>
          
        </div>
      </article>

      <footer>
        <p>Website template inspired by <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>, 
          <a href="https://lilianweng.github.io/">Lilian Weng</a> 
          and <a href="https://pascalmichaillat.org/hugo-website/?s=08">Professor Dr von Igelfeld</a>.</p>
      </footer>
    </div>
  </body>
</html> 